# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_7th_TIL

### 3부. 데이터 분석하기
### 13.머신러닝 분석 방법론
### 14.모델 평가



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | ✅      | 
|5주차| 2부 p.203~254  | ✅      | 
|6주차| 3부 p.300~356  | ✅      | 
|7주차| 3부 p.357~615  | ✅      | 

<!-- 여기까진 그대로 둬 주세요-->

# 13.머신러닝 분석 방법론

```
✅ 학습 목표 :
* 선형 회귀와 다항 회귀를 비교하고, 데이터를 활용하여 적절한 회귀 모델을 구축할 수 있다. 
* 로지스틱 회귀 분석의 개념과 오즈(Odds)의 의미를 설명하고, 분류 문제에 적용할 수 있다.
* k-means 알고리즘의 원리를 설명하고, 적절한 군집 개수를 결정하여 데이터를 군집화할 수 있다.
```

## 13.1. 선형 회귀분석과 Elastic Net(예측모델)
**회귀분석의 기원과 원리**

회귀분석(regression analysis)은 통계 분석의 꽃이라 불리며, 평균으로의 회귀(regression toward mean)에서 유래

프랜시스 골턴은 부모의 키와 자식의 키 관계를 연구하며, 부모의 키가 클수록 자식도 크지만 평균보다 작아지고, 반대로 작을수록 평균보다 커지는 경향을 발견

이 관찰을 바탕으로 회귀 개념이 등장했고, 이후 칼 피어슨이 수학적으로 정식화하며 회귀분석이 본격적으로 체계화

실제 예로, 아버지 키가 152cm일 경우 아들 평균 키는 약 164cm, 아버지 키가 188cm면 아들 평균 키는 약 183cm로 관측

이 관계를 수식으로 모델링하면 다음과 같은 1차 선형 회귀식으로 표현 가능

𝑦^=33.73+0.516𝑥

**회귀분석의 핵심 목적**

독립변수 x를 기반으로 종속변수 y의 평균값을 예측하는 것

예측 정확도를 높이기 위해 오차(잔차)의 제곱합(RSS, Residual Sum of Squares)을 최소화하는 방향으로 회귀선을 찾음

평균 키, 성별, 몸무게 등 다양한 독립변수를 활용해 예측 정확도를 향상시킬 수 있음

-> 회귀분석이란 “종속변수 y 값에 영향을 주는 독립변수들의 조건을 고려해 평균값을 추정하는 것”이라고 정의

**회귀모형 수식과 의미**

일반적인 다중 선형 회귀 모형은 다음과 같은 수식으로 표현됨

𝑦=𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + ⋯ + 𝛽𝑛𝑋𝑛+𝜖

​
β0: 절편

βi: 각 독립변수의 영향력(기울기)

ϵ: 오차항

회귀선은 절편, 기울기, 오차항으로 구성되며, 오차항은 예측되지 않은 변동성을 나타냄

회귀계수를 추정하는 방법으로는 최소제곱추정법(Least Squares Estimation)을 사용

예측값과 관측값 간의 거리(오차)의 제곱합을 최소화하는 직선이 최적의 회귀선이 됨

이는 최대우도추정법(Maximum Likelihood Estimation)과 동일한 의미를 가짐


**회귀모형의 전제조건**

정규성: 잔차는 정규분포를 따라야 함

등분산성: 잔차의 분산이 일정해야 함

독립성: 독립변수들 간에 상관성이 없어야 함 (다중공선성 없음)

선형성: 독립변수와 종속변수의 관계는 선형적이어야 함

정규성 검정 방법: 히스토그램, Q-Q plot, Shapiro-Wilk test, Anderson-Darling test 등

다중공선성 확인: VIF(Variance Inflation Factor), 공차한계(Tolerance)

**회귀분석 결과 해석 요소**

Parameter Estimate: 각 독립변수의 계수로, 1 단위 증가 시 종속변수에 미치는 영향

Intercept: 독립변수 값이 0일 때의 종속변수 기본값

Standard Error: 계수의 표준오차, 작을수록 정확도 높음

T-value: 계수가 통계적으로 유의한지를 판단하는 값 (계수 / 표준오차)

P-value: T값의 유의확률. 0.05 미만이면 일반적으로 유의한 변수로 간주

VIF: 다중공선성 존재 여부 판단 (10 이상이면 제거 고려)

**변수 선택 알고리즘**

전진 선택법 (Forward Selection)
절편만 포함한 상태에서 시작해 유의미한 변수를 하나씩 추가

후진 제거법 (Backward Elimination)
모든 변수를 포함한 상태에서 시작해 중요하지 않은 변수를 제거

단계적 선택법 (Stepwise Selection)
전진과 후진을 혼합하여 변수 추가 및 제거를 반복


LARS (Least Angle Regression): 단계적 선택법 개선 버전

유전 알고리즘: 변수 조합을 무작위로 탐색해 최적 조합 도출

**다항 회귀 (Polynomial Regression)**

선형 회귀는 직선 관계만 다룰 수 있으므로 곡선형 관계에는 부적합

독립변수와 종속변수의 관계가 곡선일 경우, 독립변수에 고차항을 추가하여 다항 회귀(polynomial regression)를 사용

과적합 위험이 커지므로 변수의 차수를 늘릴수록 검증 절차가 필수

**Elastic Net (엘라스틱넷)**

Ridge 회귀는 계수 크기에 패널티를 주어 과적합 방지 (L2 정규화)

Lasso 회귀는 계수를 아예 0으로 만들어 변수 선택 기능 포함 (L1 정규화)

Elastic Net은 이 두 가지를 결합한 방법으로, Ridge와 Lasso의 장점을 동시에 활용

변수 수가 많고 다중공선성이 있는 상황에서 특히 유용하며, 예측 정밀도 향상에 효과적임




## 13.2. 로지스틱 회귀분석 (분류모델)

**로지스틱 회귀분석 (Logistic Regression)**

**기본 개념과 선형 회귀와의 차이**

로지스틱 회귀는 분류 모델로, 종속변수가 연속형(수치)이 아닌 범주형(0 또는 1)일 때 사용

예: 구매/미구매, 합격/불합격, 성공/실패 등

선형 회귀는 수치 예측을, 로지스틱 회귀는 확률 기반 이진 분류를 수행함

종속변수 범주가 3개 이상일 경우 → 다항 로지스틱 회귀(Multinomial Logistic Regression) 사용


**로지스틱 회귀의 작동 원리**
선형 회귀식 구조 유지

𝑦=𝛽0+𝛽1𝑋1+⋯+𝛽𝑛𝑋𝑛
 
단, 이 식으로는 확률값이 무한대로 뻗어나가기 때문에 실제 확률 예측에는 부적절

-> 오즈(Odds) 개념 도입

사건이 발생할 확률 / 발생하지 않을 확률

예: 발생 확률 60%, 비발생 확률 40% → 오즈 = 0.6 / 0.4 = 1.5

오즈 > 1이면 사건 발생 가능성이 높다는 뜻

- 로짓(Logit) 변환

  오즈에 자연로그 취함:

  logit(𝑝)=log(𝑝1−𝑝)
​  
결과는 음의 무한대 ~ 양의 무한대의 범위를 가짐

- 시그모이드(Sigmoid) 함수 적용

  로짓 값을 다시 확률로 되돌리기 위해 사용

  𝑃=1+𝑒−(𝛽0+𝛽1𝑥)
 
  이로써 최종 확률값을 0과 1 사이의 S자 곡선 형태로 변환

- 분류 기준값(Threshold)

  기본 임계치는 0.5

  0.5보다 크면 1, 작으면 0으로 분류

  상황에 따라 기준값을 조정할 수 있음


**다항 로지스틱 회귀**

종속변수가 3개 이상의 범주일 때 적용

이항 로지스틱 회귀를 각 범주별로 반복 수행

하나의 기준 범주를 정하고 나머지 범주들과 비교해 확률 계산

각 범주에 속할 확률의 총합은 1로 유지됨

**회귀계수의 해석과 오즈비(Odds Ratio)**

계수는 오즈에 대한 영향력을 의미함

예: X1의 오즈비가 1.223이면, X1이 1 증가할 때 종속변수가 1일 확률이 1.223배 커짐

계수가 음수이고 오즈비 < 1이면, 해당 변수는 종속변수 1의 가능성을 낮춤

더미 변수(0 또는 1인 변수)일 경우에도 오즈비로 해석 가능

**로지스틱 회귀의 적합도 평가 (R² 대체 방법)**

선형회귀처럼 R²(결정계수)를 그대로 적용할 수 없음

다양한 pseudo-R² 지표가 존재하며, 대표적 방식은 Tjur(2009)의 방법

예측된 1인 그룹의 평균 확률과 0인 그룹의 평균 확률의 차이

차이가 클수록 모델의 구분력이 높다는 뜻

그 외에도 McFadden R², Cox & Snell R², Nagelkerke R² 등 다양한 방식이 존재



## 13.8. k-means 클러스터링(군집모델)
**K-means 클러스터링 분석**

K-means 클러스터링은 비지도 학습(unsupervised learning) 알고리즘

KNN(K-Nearest Neighbors)은 지도 학습이므로 학습 시 정답(레이블)이 필요하지만, K-means는 정답 없이 데이터의 구조나 패턴을 자율적으로 찾아냄

KNN은 분류(Classification)이고, K-means는 군집화(Clustering)임

클러스터링은 고객 세그먼트 분석, 마케팅 타겟 분류 등에 활용됨

**K-means의 핵심 원리**

k: 군집 개수

means: 각 군집의 중심점(Centroid)

관측치들과 중심점 간의 유클리드 거리를 기반으로 군집을 반복적으로 재구성

군집 내의 거리합(SSE: Sum of Squared Errors)을 최소화하는 방향으로 중심점을 이동


**알고리즘 동작 절차 (반복적 최적화)**

1. 초기 중심점 k개 무작위로 선정

2. 각 관측치와 중심점 간 유클리드 거리 계산

3. 가장 가까운 중심점에 관측치 할당

4. 각 군집 내의 평균 위치를 새 중심점으로 갱신

5. 중심점이 이동하지 않을 때까지 2~4단계 반복

6. 이 과정을 통해 중심점과 군집 구조가 수렴되면 알고리즘 종료

**지역 최솟값(Local Minimum) 문제**

초기 중심점 배치에 따라 잘못된 군집화 결과(비최적) 발생 가능

알고리즘이 전역 최솟값(Global minimum)에 도달하기 전에 지역 최솟값에서 수렴될 수 있음

이를 해결하기 위한 방법

랜덤 초기화 반복 수행 후 가장 좋은 결과 선택

- k-means++: 첫 중심점을 무작위로 고르고, 이후 중심점은 기존 중심점들과 멀리 떨어진 데이터를 높은 확률로 선택해 초기 중심점 분산 확보


**적절한 k값(군집 수) 결정 방법**

- 도메인 지식 기반 판단

  기업 내부 경험, 산업적 맥락에 따라 군집 개수 지정

  해석력은 높지만 데이터 기반 객관성은 낮을 수 있음

- 엘보우 기법 (Elbow Method)

  군집 수(k)에 따른 거리합(Inertia)의 변화량을 관찰

  거리합 감소가 급격하게 완화되는 지점이 적정 k

  그 이후 k 증가에 따른 성능 개선이 미미하다면 그 시점이 최적

- 실루엣 계수 (Silhouette Coefficient)

  1에 가까울수록 군집화 품질이 우수

  계산식:𝑆𝑖=𝑏𝑖−𝑎𝑖/max(𝑎𝑖,𝑏𝑖)
 
  𝑎𝑖: 동일 군집 내 평균 거리

  𝑏𝑖: 가장 가까운 다른 군집과의 평균 거리

  군집 내 응집도 높고 군집 간 분리가 클수록 높은 값을 가짐

**클러스터링 전처리의 중요성**

K-means는 거리 기반 알고리즘이므로 각 변수의 크기에 영향을 받음

반드시 스케일 조정 필요 (표준화 또는 정규화)

예: 키(cm)와 나이(세)의 단위 차이로 인해 군집 결과가 왜곡될 수 있음



# 14. 모델 평가

```
✅ 학습 목표 :
* 유의확률(p-value)을 해석할 때 주의할 점을 설명할 수 있다.
* 분석가가 올바른 주관적 판단을 위한 필수 요소를 식별할 수 있다.
```

## 14.3. 회귀성능 평가지표
- 결정계수 (R², Adjusted R²)
R² (결정계수)

회귀모형이 종속변수의 변동을 얼마나 설명하는지를 나타냄

범위: 0 ~ 1 (1에 가까울수록 설명력 높음)

수식:
𝑅2=1−𝑆𝑆𝐸/𝑆𝑆𝑇=𝑆𝑆𝑅/𝑆𝑆𝑇

 
SST: 총 제곱합 (전체 데이터의 변동성)

SSR: 회귀식이 설명한 제곱합

SSE: 잔차 제곱합


- Adjusted R² (보정된 결정계수)

변수 개수가 많을수록 R²가 자동으로 증가하는 현상을 보정

자유도를 고려한 평가 지표

수식:
Adjusted 𝑅2=1−(𝑆𝑆𝐸/(𝑛−𝑘−1)𝑆𝑆𝑇/(𝑛−1))

𝑛: 샘플 수, 
𝑘: 독립 변수 수


- RMSE (Root Mean Square Error)

예측값과 실제값의 차이를 제곱하여 평균 후 루트를 씌운 값

예측 오차의 표준편차로 해석 가능

수식:

𝑅𝑀𝑆𝐸=1𝑛∑𝑖=1𝑛(𝑦𝑖−𝑦^𝑖)2

특징: 이상치에 민감함, 값의 단위와 스케일에 영향 받음
 
​
- MAE (Mean Absolute Error)

예측값과 실제값의 절댓값 차이의 평균

수식:
𝑀𝐴𝐸=1𝑛∑𝑖=1𝑛∣𝑦𝑖−𝑦^𝑖∣

RMSE보다 이상치에 덜 민감함

단위에 영향을 받으며, 직관적 해석이 용이


- MAPE (Mean Absolute Percentage Error)

예측 오차를 백분율로 나타낸 지표

수식:
𝑀𝐴𝑃𝐸=100𝑛∑𝑖=1𝑛∣𝑦𝑖−𝑦^𝑖𝑦𝑖∣
 
 
스케일 독립적 → 모델 간 비교에 적합

주의사항

𝑦𝑖=0
인 경우 정의되지 않음

𝑦𝑖
가 작을수록 값이 과도하게 커짐

과소 예측 시 최대 100%, 과대 예측 시 무한대 가능 → 편향 유발

- RMSLE (Root Mean Square Logarithmic Error)

RMSE에 로그 변환을 추가한 방식

수식:
𝑅𝑀𝑆𝐿𝐸=1𝑛∑𝑖=1𝑛(log(𝑦𝑖+1)−log(𝑦^𝑖+1))2

비율적 오차에 기반하여 스케일 독립적

이상치에 RMSE보다 덜 민감

 - AIC (Akaike Information Criterion)
최대우도(likelihood)에 변수 수에 대한 패널티를 적용한 모델 평가 지표

수식:
𝐴𝐼𝐶=−2log(𝐿𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑)+2𝑘

k: 독립 변수 개수

작을수록 좋은 모델

과적합 방지 목적 → 변수 수 많으면 패널티 커짐

관측치 수 반영하지 않음 → 데이터가 적을 경우 AICc 사용 권장

- BIC (Bayesian Information Criterion)
AIC와 유사하지만 관측치 수를 고려한 더 강한 패널티 적용

수식:
𝐵𝐼𝐶=−2log(𝐿𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑)+𝑘log(𝑛)

관측치 𝑛이 많을수록 변수 수에 더 큰 패널티

변수 수 줄이는 데 유리

단순한 모델 선호 시 AIC보다 더 적합


## 14.6. 유의확률의 함정

유의확률(P-value)의 함정과 한계

**P값의 정의와 일반적 사용**

P값의 의미:

귀무가설(null hypothesis)이 참일 때, 관측된 통계량과 같거나 더 극단적인 결과가 나올 확률

통상적 기준:

P < 0.05 → 통계적으로 유의하다 판단

하지만 이 기준은 임의적 관례에 불과함 (피셔가 예시로 제시했던 수치가 굳어진 것뿐)

**P값의 오해와 잘못된 해석**

“P값이 0.05면 연구가설이 맞을 확률이 95%다”	❌

-> 귀무가설이 참일 경우, 현재 결과가 나올 확률이 5%다

“P값이 작으면 효과가 크다”	❌

-> P값은 효과의 크기나 중요성과 무관하다

“P값이 크면 귀무가설이 맞다” ❌ 

->P값은 단지 귀무가설을 기각할 만한 증거가 부족하다는 뜻일 뿐이다

**P값의 주요 한계**

- 표본 수에 민감

  표본 수가 커질수록 작은 차이도 유의하게 나와 P값은 자연히 작아짐

  큰 데이터셋에서는 실제로는 미미한 효과도 통계적으로 유의하다고 판단됨

- 기계적인 기준 적용의 위험

  단순히 “P < 0.05면 유의”라는 이분법적 판단은 오판 가능성 높음

  과학적/사업적/정책적 결정은 효과 크기, 외부 증거, 맥락 등을 함께 고려해야 함

- 반복성 재현의 실패 문제

  P값 0.05 이내로 유의했던 실험도 재현 시 다른 결과가 나오는 경우 많음

  이로 인해 최근에는 기준을 0.005로 강화하자는 주장이 늘고 있음

  0.005 기준은 위양성(false positive) 발생률을 현저히 낮출 수 있음

-  효과의 크기와 중요성 무시

   P값은 단지 “관측값이 우연일 확률”일 뿐

   효과의 강도나 중요성은 회귀 계수, 상관계수, 표준화 계수 등으로 판단해야 함




## 14.7. 분석가의 주관적 판단과 스토리텔링

**데이터 분석에 왜 ‘사람’이 필요한가?**

아무리 정교한 AI·모델링을 사용해도, 사람의 지식과 판단이 배제된 분석은 실패 가능성이 높음

데이터에는 보이지 않는 심리, 문화, 맥락이 존재하고, 이는 알고리즘이 파악하지 못함

분석가는 데이터 이면의 의미를 해석하고, 분석 결과를 올바르게 해석하고 수정하는 역할을 해야 함

**사례로 보는 주관적 판단의 필요성**

- 사례 ① 멕시코 자동차 C 차종

      문제: 타깃은 2030대 여성인데, 예측 결과는 5060대 남성

      분석가의 판단: 멕시코 문화상 부모 세대가 자녀 차를 대신 구입하는 경향을 반영해야 함

      교훈: 실제 구매 주체와 의사결정 주체가 다를 수 있으므로 데이터 이면의 맥락을 파악해야 함

- 사례 ② 통신사 약정 종료 고객 프로모션

      문제: 프로모션을 받은 고객이 오히려 이탈률이 더 높음

      원인: 무심코 서비스를 사용하던 고객에게 해지 가능성을 오히려 인식시킴

      조치: 이탈 가능성이 높은 고객만 선별해 프로모션 → 불필요한 자극 최소화

      교훈: 행동 유도는 심리적 트리거 효과를 고려해야 하며, 종속변수의 반응도 시뮬레이션해야 함

**분석가에게 필요한 세 가지 판단 기반**

① 도메인 지식	

분석 대상 산업/업무 흐름을 깊이 이해해야 타당한 해석 가능

② 통계적 분석 능력	

EDA 및 전처리 과정에서 이상치, 누락, 분포 왜곡 등을 판단할 수 있어야 함

③ 커뮤니케이션 및 검증	

중간 결과를 타 부서와 공유하고, A/B 테스트 등 사전 검증 과정을 거쳐야 신뢰 확보 가능

**스토리텔링이 중요한 이유**
분석 결과는 설득력이 있어야 의미가 있음

듣는 사람이 왜 이 분석이 중요한지, 어떤 변화를 가져왔는지를 공감할 수 있어야 함

단순히 모델 성능이나 수치 나열이 아니라, 배경–문제–해결의 구조가 있어야 함

**데이터 분석 스토리텔링 3막 구조**
- 1막: 서론 – 배경과 문제 인식

   문제 상황을 구체적인 수치로 제시 → 청중의 공감과 관심 확보

   예시: “A제품 불량률 5% → 연간 손실 100억 원”


- 2막: 본론 – 분석 과정과 접근

   데이터 수집 방식, 분석 기간, 변수 구성 등 신뢰를 줄 수 있는 정보 제공

   수치나 그래프는 핵심 내용만 단순하게 전달

   전문 용어나 수식은 되도록 배제 → 질문 시 보조자료 형태로 준비

- 3막: 결론 – 결과 및 향후 방향

   개선 수치, 성과 정리 (ex: 불량률 1%로 감소 → 연 90억 절감)

   중립적인 어조, 한계점 명시 → 신뢰도 상승

   향후 과제나 고도화 방향을 함께 제시하면 더 효과적

**로그라인(log-line): 한 문장 요약의 힘**

전체 프로젝트 내용을 문제와 해결 구조로 압축한 문장

예시:

“5%의 불량률로 연간 100억 원 손실이 발생하는 A 제품 공정을, 센서 분석을 통해 1%로 개선하여 90억 원 절감했다”

로그라인은 스토리 정합성 점검 도구이자 최고 전달력 요약임

<br>
<br>

# 확인 문제

## **문제 1. 선형 회귀**

> **🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.**  
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요. 
>  
> **ŷ = 33.73 + 0.516X**  
>   
> - **X**: 아버지의 키 (cm)  
> - **ŷ**: 아들의 예상 키 (cm)  

```
기울기 0.516의 의미:
기울기 0.516은 아버지의 키가 1cm 증가할 때마다 아들의 예측 키가 평균적으로 0.516cm 증가한다는 뜻

즉, 아버지의 키가 클수록 아들도 키가 클 경향이 있지만, 완전한 1:1 비례 관계는 아니며 평균적으로 더 작게 증가한다는 점이 핵심

추가 해석 – 평균으로의 회귀(regression to the mean)
이 계수 0.516은 1보다 작기 때문에, 아버지 키가 매우 클수록 아들 키는 아버지보다 평균 쪽으로 회귀하는 경향이 있음을 의미

예를 들어:

아버지가 평균보다 10cm 크다면, 아들은 평균보다 약 5.16cm 정도만 클 가능성이 높음

이는 키가 유전되기는 하지만, 완전히 유전되지 않고 평균에 가까워지는 경향이 있다는 것을 보여주는 대표적인 회귀 사례

```
---

## **문제 2. 로지스틱 회귀**  

> **🧚 다트비에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 '낮음' 입니다.**   

| 변수 | Odds Ratio Estimates | 95% Wald Confidence Limits |  
|------|----------------------|--------------------------|  
| Study Hours | **2.34** | (1.89, 2.88) |  
| Attendance Rate | **3.87** | (2.92, 5.13) |  

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.

<!--변수 Study Hours의 오즈비 값이 2.34라는 것과 Attendance Rate의 오즈비 값이 3.87이라는 것이 각각 무엇을 의미하는지 구체적으로 생각해보세요.-->

```
오즈비(Odds Ratio)는 독립변수가 한 단위 증가할 때, 종속변수가 기준범주(‘낮음’) 대비 다른 범주(‘보통’ 또는 ‘높음’)에 속할 가능성(오즈)이 몇 배로 변하는지를 나타낸다

Study Hours의 오즈비 2.34는, 주당 공부 시간이 1시간 증가할 때 ‘보통’ 또는 ‘높음’ 학업 성취도일 오즈가 ‘낮음’일 오즈보다 2.34배 증가함을 의미함

Attendance Rate의 오즈비 3.87은 출석률이 1단위(예: 1%) 증가할 때 학업 성취도가 ‘보통’ 또는 ‘높음’일 오즈가 3.87배 증가함을 의미함

즉, 공부 시간과 출석률이 높을수록 낮은 학업 성취도보다 더 높은 성취도를 보일 확률이 기하급수적으로 증가한다는 뜻이다
```

> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.
<!--각 변수의 신뢰구간에 제시된 수치가 의미하는 바를 생각해보세요.-->

```
Wald 신뢰구간(Confidence Limits)은 추정된 오즈비의 신뢰 가능한 범위를 말한다

예를 들어, Study Hours의 95% 신뢰구간이 (1.89, 2.88)이라는 것은 오차를 고려하더라도, 95%의 신뢰수준 하에서 Study Hours가 오즈에 미치는 영향은 최소 1.89배, 최대 2.88배일 것이다

마찬가지로 Attendance Rate의 신뢰구간 (2.92, 5.13)은 그 영향력이 매우 강하고 **통계적으로 유의함(P < 0.05)을 의미 (신뢰구간에 1이 포함되지 않기 때문)
```

> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
<!--Study Hours와 Attendance Rate 중 어느 변수가 학업 성취도에 더 큰 영향을 미치는지를 고려하여, 학업 성취도를 향상시키기 위한 효과적인 전략을 구체적으로 제시해보세요.-->

```
출석률 제고 프로그램 도입

지각/결석 감점제도 강화, 출석 리워드 시스템 도입

공부 시간 증진 환경 조성

자율학습 공간 확충, 방과 후 스터디 운영, 공부 시간 추적 앱 도입

출석률이 낮은 학생: 우선 출석 행동 개선부터 개입

학생 맞춤형 개입과 상담 연계

출석률과 공부시간 기반 클러스터링 → 위험군 조기 식별 및 학습 상담

피드백과 동기 강화


```

---


## **문제 3. k-means 클러스터링**

> **🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.**

```
- 엘보우 기법 (Elbow Method)

군집 수를 증가시키며 각 k값에 대해 **군집 내 제곱합(SSE, inertia)**을 계산

군집 수가 늘수록 SSE는 감소하지만 급격한 감소가 멈추는 지점(knee) 이후부터는 개선 효과가 미미

실행 방법:

여러 k값에 대해 K-means 실행

각 k에 대한 SSE 값을 그래프로 시각화

SSE 곡선에서 팔꿈치처럼 꺾이는 지점(knee point)을 최적의 군집 수로 선택

- 실루엣 계수 (Silhouette Coefficient)


각 데이터 포인트가 얼마나 잘 자기 군집에 속해 있고, 다른 군집과는 얼마나 구분되는지를 평가

값의 범위는 -1 ~ 1

1에 가까울수록 군집 내부 응집도가 높고 외부 군집과의 분리가 명확

실행 방법:

여러 k값에 대해 클러스터링 수행

각 경우마다 전체 실루엣 계수의 평균값 계산

가장 높은 실루엣 계수를 갖는 k값을 선택

```

### 🎉 수고하셨습니다.
